{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./assets/iland_core_logo.jpg\" style=\"{width: 150px; height: 150px;}\">\n",
    "<img src=\"./assets/spark-logo-trademark.png\" style=\"{float: left}\">\n",
    "</div>\n",
    "\n",
    "<h1>Setting Up Your Spark Environment</h1>\n",
    "\n",
    "<h3>Downloading Spark</h3>\n",
    "\n",
    "<a href=\"https://spark.apache.org/\">Link to Spark's website</a>\n",
    "\n",
    "<p>Use the tar command to extract <br> </p>\n",
    "<code>tar -xzf /path/to/file</code>\n",
    "\n",
    "<h3>Spark Documentation</h3>\n",
    "<a href=\"https://spark.apache.org/docs/latest/\" target=\"_blank\">Spark Documentation</a><br>\n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\">PySpark Documentation</a>\n",
    "\n",
    "You can play around in an interactive Spark Shell by doing\n",
    "<code>$SPARK_HOME/bin/pyspark</code>\n",
    "\n",
    "Or if you are feeling brave enough to jump into Scala, then\n",
    "<code>$SPARK_HOME/bin/spark-shell</code>\n",
    "\n",
    "<h3>Running a Spark Script</h3>\n",
    "\n",
    "You can run Spark jobs through the submit in a bash file:\n",
    "\n",
    "<code>/path/to/spark/bin/spark-submit \\\n",
    "    --master local[*] \\\n",
    "    path/to/script.py \n",
    "</code>\n",
    "\n",
    "<p>The point of Spark is to leverage the computing and memory of a cluster of nodes. Here is a link for how to have Spark point to your cluster management tool.\n",
    "    <a href=\"https://spark.apache.org/docs/latest/submitting-applications.html#master-urls\">Spark Master's</a>\n",
    "</p>\n",
    "\n",
    "\n",
    "<h3>Packages and Configurations</h3>\n",
    "    \n",
    "<p>You can use flags to add packages and configurations to your spark job. <a href=\"https://spark.apache.org/docs/latest/configuration.html\" target=\"_blank\">Spark Configurations</a></p>\n",
    "\n",
    "\n",
    "<code>path/to/spark/bin/spark-submit \\\n",
    "        --master mesos://MESOS_IP_VARIABLE:7077 \\\n",
    "        --conf spark.mesos.coarse=true \\\n",
    "        --executor-memory 1G \\\n",
    "        --packages com.datastax.spark:spark-cassandra-connector_2.11:2.4.1 \\\n",
    "    path/to/script.py\n",
    "</code>\n",
    "    \n",
    "\n",
    "<h3>Driver's</h3>\n",
    "\n",
    "<p>Reading and writing to a SQL database requires a JDBC driver, which is straightforward when running Spark through spark-submit</p>\n",
    "\n",
    "<code>/path/to/spark/bin/spark-submit \\\n",
    "    --master local[*] \\\n",
    "    --packages com.datastax.spark:spark-cassandra-connector_2.11:2.4.1 \\\n",
    "    --driver-class-path /path/to/jar/file.jar --jars path/to/jar/file.jar\n",
    "    path/to/script.py \n",
    "</code>\n",
    "\n",
    "<h2>Â¡VERY IMPRORTANT!</h2>\n",
    "<h3>Special Cases for Jupyter</h3>\n",
    "\n",
    "<p>You will need to set the SPARK_HOME path for Jupyter to find Spark (in the terminal)</p>\n",
    "<code>export SPARK_HOME=/path/to/spark</code>\n",
    "\n",
    "<p>To make this work properly in a Jupyter Notebook, you need to something like this in the Notebook itself (for PostgreSQL)</p>\n",
    "\n",
    "<code>import os\n",
    " os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.postgresql:postgresql:42.2.5 pyspark-shell'\n",
    "</code>\n",
    "\n",
    "<p>Then when reading or writing to a SQL Database</p>\n",
    "\n",
    "<code>spark.read.format(\"jdbc\").option(\"driver\",\"org.postgresql.Driver\")...</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Spark Basics</h1>\n",
    "\n",
    "\n",
    "<h3>Spark Libraries</h3>\n",
    "<ul>\n",
    "    <li><strong>Spark SQL</strong></li>\n",
    "    <li><strong>Spark Streaming</strong></li>\n",
    "    <li>MLLib</li>\n",
    "    <li>GraphX</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Spark Session</h2>\n",
    "\n",
    "<p>The Spark Session is how your script interacts with Spark.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"Flatiron Presentation\") \\\n",
    "    .setMaster(\"spark://asprague-laptop:7077\") \\\n",
    "    .set(\"spark.driver.allowMultipleContexts\", True) \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>RDDs</h2>\n",
    "\n",
    "<p>RDDs, or Resilient Distributed Datasets, are the basis of Spark. A lot of the RDD functionality had been abstracted away by DataFrames, so I will not spend too much time on RDDs. But here is a small example.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_names = [{'first_name': 'Aegon', 'last_name': 'Targaryen'},\n",
    "              {'first_name': 'Daenerys', 'last_name': 'Targaryen'},\n",
    "              {'first_name': 'Arya', 'last_name': 'Stark'},\n",
    "              {'first_name': 'Sansa', 'last_name': 'Stark'},\n",
    "              {'first_name': 'Ned', 'last_name': 'Stark'}]\n",
    "\n",
    "\n",
    "got_character_rdd = spark.sparkContext.parallelize(list_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_name': 'Aegon', 'last_name': 'Targaryen'}\n",
      "{'first_name': 'Daenerys', 'last_name': 'Targaryen'}\n",
      "{'first_name': 'Arya', 'last_name': 'Stark'}\n",
      "{'first_name': 'Sansa', 'last_name': 'Stark'}\n",
      "{'first_name': 'Ned', 'last_name': 'Stark'}\n"
     ]
    }
   ],
   "source": [
    "# Collect in for loop to print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>A note on Lazy Evaluation</h3>\n",
    "\n",
    "<p>Spark does not execute until an action is triggered.</p>\n",
    "<ul>\n",
    "    <li>Increases Manageability</li>\n",
    "    <li>Saves Computation and increases Speed</li>\n",
    "    <li>Reduces Complexities</li>\n",
    "    <li>Optimization</li>\n",
    "    \n",
    "</ul>\n",
    "<p>Source: <a href=\"https://data-flair.training/blogs/apache-spark-lazy-evaluation/\">Data Flair</a>\n",
    "    \n",
    "<h3>Count the people with the same last name</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Targaryen', 2)\n",
      "('Stark', 3)\n"
     ]
    }
   ],
   "source": [
    "character_family_count = got_character_rdd.map(lambda x: (x['last_name'], 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "for house in character_family_count.collect():\n",
    "    print(house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DataFrames</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://spark.apache.org/docs/2.1.2/api/python/_modules/pyspark/sql/types.html\" target=\"_blank\">PySpark Types</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "\n",
    "\n",
    "got_character_schema = StructType([\n",
    "    StructField(\"first_name\", StringType(), False),\n",
    "    StructField(\"last_name\", StringType(), False),\n",
    "])\n",
    "\n",
    "got_character_df = spark.createDataFrame(list_names, schema=got_character_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|last_name|count|\n",
      "+---------+-----+\n",
      "|Targaryen|    2|\n",
      "|    Stark|    3|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "got_character_df.groupBy(\"last_name\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading From Other File Types</h3>\n",
    "\n",
    "<ul>\n",
    "    <li>Text</li>\n",
    "    <li>CSV</li>\n",
    "    <li>JSON</li>\n",
    "    <li>Parquet (Columnar Storage)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading from JSON</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+--------+--------+\n",
      "|first_name|last_name|start_date|end_date|meetings|\n",
      "+----------+---------+----------+--------+--------+\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "|      null|     null|      null|    null|    null|\n",
      "+----------+---------+----------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DateType, ArrayType, DecimalType, BooleanType\n",
    "\n",
    "fed_chair_path = \"docs/fed_chairs.json\"\n",
    "\n",
    "fed_schema = StructType([\n",
    "    StructField(\"first_name\", StringType(), False),\n",
    "    StructField(\"last_name\", StringType(), False),\n",
    "    StructField(\"start_date\", StringType(), False),\n",
    "    StructField(\"end_date\", StringType(), False),\n",
    "    StructField(\"meetings\",ArrayType(StructType([\n",
    "        StructField(\"date\", StringType(), False),\n",
    "        StructField(\"current_fed_funds\", StructType([\n",
    "            StructField(\"upper\", DecimalType(6,4), False),\n",
    "            StructField(\"lower\", DecimalType(6,4), False)\n",
    "        ]),False),\n",
    "        StructField(\"new_fed_funds\", StructType([\n",
    "            StructField(\"upper\", DecimalType(6,4), False),\n",
    "            StructField(\"lower\", DecimalType(6,4), False)\n",
    "        ]), False), \n",
    "        StructField(\"rate_change\", BooleanType(), False)\n",
    "    ]), False), False)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "spark.read.json(fed_chair_path, schema=fed_schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading from CSV</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobs.ncaa_basketball_schema import bb_schema\n",
    "\n",
    "bb_event_path = \"./docs/pbp000000000000.csv\"\n",
    "\n",
    "bb_event_df = spark.read \\\n",
    "    .csv(bb_event_path, header=True, schema=bb_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- game_id: string (nullable = true)\n",
      " |-- load_timestamp: string (nullable = true)\n",
      " |-- season: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- scheduled_date: string (nullable = true)\n",
      " |-- venue_id: string (nullable = true)\n",
      " |-- venue_name: string (nullable = true)\n",
      " |-- venue_city: string (nullable = true)\n",
      " |-- venue_state: string (nullable = true)\n",
      " |-- venue_address: string (nullable = true)\n",
      " |-- venue_zip: string (nullable = true)\n",
      " |-- venue_country: string (nullable = true)\n",
      " |-- venue_capacity: integer (nullable = true)\n",
      " |-- attendance: integer (nullable = true)\n",
      " |-- neutral_site: boolean (nullable = true)\n",
      " |-- conference_game: boolean (nullable = true)\n",
      " |-- tournament: string (nullable = true)\n",
      " |-- tournament_type: string (nullable = true)\n",
      " |-- round: string (nullable = true)\n",
      " |-- game_no: string (nullable = true)\n",
      " |-- away_market: string (nullable = true)\n",
      " |-- away_name: string (nullable = true)\n",
      " |-- away_id: string (nullable = true)\n",
      " |-- away_alias: string (nullable = true)\n",
      " |-- away_conf_name: string (nullable = true)\n",
      " |-- away_conf_alias: string (nullable = true)\n",
      " |-- away_division_name: string (nullable = true)\n",
      " |-- away_division_alias: string (nullable = true)\n",
      " |-- away_league_name: string (nullable = true)\n",
      " |-- home_market: string (nullable = true)\n",
      " |-- home_name: string (nullable = true)\n",
      " |-- home_id: string (nullable = true)\n",
      " |-- home_alias: string (nullable = true)\n",
      " |-- home_conf_name: string (nullable = true)\n",
      " |-- home_conf_alias: string (nullable = true)\n",
      " |-- home_division_name: string (nullable = true)\n",
      " |-- home_division_alias: string (nullable = true)\n",
      " |-- home_league_name: string (nullable = true)\n",
      " |-- period: integer (nullable = true)\n",
      " |-- game_clock: string (nullable = true)\n",
      " |-- elapsed_time_sec: string (nullable = true)\n",
      " |-- possession_arrow: string (nullable = true)\n",
      " |-- team_name: string (nullable = true)\n",
      " |-- team_market: string (nullable = true)\n",
      " |-- team_id: string (nullable = true)\n",
      " |-- team_alias: string (nullable = true)\n",
      " |-- team_conf_name: string (nullable = true)\n",
      " |-- team_conf_alias: string (nullable = true)\n",
      " |-- team_division_name: string (nullable = true)\n",
      " |-- team_division_alias: string (nullable = true)\n",
      " |-- team_league_name: string (nullable = true)\n",
      " |-- team_basket: string (nullable = true)\n",
      " |-- posession_team_id: string (nullable = true)\n",
      " |-- player_id: string (nullable = true)\n",
      " |-- player_full_name: string (nullable = true)\n",
      " |-- jersey_num: integer (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- event_description: string (nullable = true)\n",
      " |-- event_coord_x: integer (nullable = true)\n",
      " |-- event_coord_y: integer (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- shot_made: boolean (nullable = true)\n",
      " |-- shot_type: string (nullable = true)\n",
      " |-- shot_subtype: string (nullable = true)\n",
      " |-- three_point_shot: boolean (nullable = true)\n",
      " |-- points_scored: integer (nullable = true)\n",
      " |-- turnover_type: string (nullable = true)\n",
      " |-- rebound_type: string (nullable = true)\n",
      " |-- timeout_duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bb_event_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-------------+----------+\n",
      "|game_id                             |home_name    |away_name |\n",
      "+------------------------------------+-------------+----------+\n",
      "|60f60dfe-4423-4ef2-9b6c-aee096a0b65c|Bears        |Cowboys   |\n",
      "|60f60dfe-4423-4ef2-9b6c-aee096a0b65c|Bears        |Cowboys   |\n",
      "|5be9a30e-3582-465d-b19e-5388eddbacd5|Trojans      |Musketeers|\n",
      "|5be9a30e-3582-465d-b19e-5388eddbacd5|Trojans      |Musketeers|\n",
      "|5be9a30e-3582-465d-b19e-5388eddbacd5|Trojans      |Musketeers|\n",
      "|5be9a30e-3582-465d-b19e-5388eddbacd5|Trojans      |Musketeers|\n",
      "|63d257d7-4865-49d1-afd1-fd4ca560bbd5|Miners       |Hawkeyes  |\n",
      "|63d257d7-4865-49d1-afd1-fd4ca560bbd5|Miners       |Hawkeyes  |\n",
      "|679a305f-2359-4e0f-97ca-3d886ca3d1c3|Demon Deacons|Volunteers|\n",
      "|7bee62f8-1a9f-4ab9-896c-9200ef45eea8|Jayhawks     |Miners    |\n",
      "|846c1adc-fb8e-47a6-8795-758b4c3fe05d|Musketeers   |Hawkeyes  |\n",
      "|846c1adc-fb8e-47a6-8795-758b4c3fe05d|Musketeers   |Hawkeyes  |\n",
      "|846c1adc-fb8e-47a6-8795-758b4c3fe05d|Musketeers   |Hawkeyes  |\n",
      "|846c1adc-fb8e-47a6-8795-758b4c3fe05d|Musketeers   |Hawkeyes  |\n",
      "|846c1adc-fb8e-47a6-8795-758b4c3fe05d|Musketeers   |Hawkeyes  |\n",
      "|846c1adc-fb8e-47a6-8795-758b4c3fe05d|Musketeers   |Hawkeyes  |\n",
      "|bbea1daf-5eaa-4218-a47a-5092a6644344|Jayhawks     |Wildcats  |\n",
      "|bbea1daf-5eaa-4218-a47a-5092a6644344|Jayhawks     |Wildcats  |\n",
      "|bbea1daf-5eaa-4218-a47a-5092a6644344|Jayhawks     |Wildcats  |\n",
      "|c70ae887-2ffa-4888-b97e-cb7a99c74b88|Wildcats     |Trojans   |\n",
      "+------------------------------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "bb_event_df.select(col(\"game_id\"),col(\"home_name\"),col(\"away_name\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StructType([\n",
    "    StructField(\"upper\", DecimalType(6,4), False),\n",
    "    StructField(\"lower\", DecimalType(6,4), False),\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
